{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "X = np.load('data/X_train.npy')#[0:load_n]\n",
    "y = np.genfromtxt('data/y_1.csv', delimiter='\\n')#[0:load_n]\n",
    "#data_test = np.load('data/X_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import random\n",
    "random.seed(99)\n",
    "\n",
    "class BinsExtraction(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Build n bins with mean from values\"\"\"\n",
    "    def __init__(self, bin_length=3,\n",
    "        images_x_from=False, images_x_to=False,\n",
    "        images_y_from=False, images_y_to=False):\n",
    "        #self.bin_length = bin_length\n",
    "\n",
    "        self.images_x_from = images_x_from\n",
    "        self.images_x_to = images_x_to\n",
    "        self.images_y_from = images_y_from\n",
    "        self.images_y_to = images_y_to\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = []\n",
    "        split_x = 10\n",
    "        split_y = 10\n",
    "        hist_bins = [1,  316.06548606,   516.04062473,   672.03912953,   788.4017923 ,\n",
    "          912.68082233,  1063.06793716,  1221.11834124,  1370.37759073, 1700]#,\n",
    "           #        100, 284.2603614 ,   461.84852282,   647.97452156,   826.76038816,\n",
    "         # 980.3224361 ,  1135.55237028,  1308.37312973,  1473.53302531])\n",
    "    \n",
    "        first = True\n",
    "        for row in X:\n",
    "            # use only without \"RemoveEmptyValues\"\n",
    "            # This is feature selection actually\n",
    "            if self.images_x_from is not False and self.images_x_to is not False:\n",
    "                #images = np.split(row, 176)[50:130] # pretty optimal already\n",
    "                images = np.split(row, 176)[self.images_x_from : self.images_x_to]\n",
    "                \n",
    "                # x needs to be set for this, but don't mind at the moment\n",
    "                #if self.images_y_from is not False and self.images_y_to is not False:\n",
    "                #    images_new = []\n",
    "                #    for image in images:\n",
    "                #        images_new.append(np.split(image, 208)[self.images_y_from : self.images_y_to])\n",
    "                #    images = np.array(images_new)\n",
    "\n",
    "                row = np.array(images).flatten()\n",
    "                #features = []\n",
    "                #for image in images:\n",
    "                #    for split in np.array_split(image, 104):  \n",
    "                #        features.append(np.histogram(split, bins=hist_bins, density=False)[0])\n",
    "\n",
    "            splits = np.array_split(row, int(len(row) / 250)) # 250\n",
    "            features = []\n",
    "            for split in splits:\n",
    "                features.append(np.histogram(split, bins=hist_bins)[0])\n",
    "\n",
    "            X_new.append(np.array(features).flatten())\n",
    "            if first:\n",
    "                print(\"features: \" + str(len(X_new[0])))\n",
    "                first = False\n",
    "\n",
    "        return X_new\n",
    "    \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('BinsExtraction', BinsExtraction(\n",
    "        bin_length=50, images_x_from=50, images_x_to=130)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('vct', VarianceThreshold(threshold=0.1)),\n",
    "    #('linreg', LinearRegression(n_jobs=-1))\n",
    "    #('kernelRidge', KernelRidge(kernel='polynomial'))\n",
    "    ('linearSVR', LinearSVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 105426\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "   'BinsExtraction__bin_length': [50],\n",
    "}\n",
    "grid = GridSearchCV(pipe, parameters, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "print(str(np.mean(grid.cv_results_['mean_test_score'])) + ', ' + str(np.mean(grid.cv_results_['std_test_score'])))\n",
    "\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pipe.fit(X[0:150], y[0:150])\n",
    "y_pr = pipe.predict(X[151:228])\n",
    "print(mean_squared_error(y[151:228], y_pr))\n",
    "\n",
    "# 52.5900323473 var0.05 bins=10\n",
    "# 51.6259478991 better bins 41k\n",
    "# 51.3320775803 added 1-edges, 47k, /500\n",
    "# 51.6 /300, 78k\n",
    "# 52.2 /220, 106k\n",
    "# 55.5 /64, 366k\n",
    "# 51.1 linearsvr, /500, 53k\n",
    "# 51.6 /510\n",
    "# 51.5 /490\n",
    "# /250\n",
    "# 52.0 add random edge, 58k\n",
    "# 51.7 remove last edge, 47k\n",
    "# 52.1 /125, 187k\n",
    "# 52.8160909792 more better bins 99k\n",
    "# 67.7           kernelridge\n",
    "# 51.5455457395 much more bins 94k\n",
    "# 51.5211591462 less bins 31k\n",
    "# 51.6344647532 more bins 67k\n",
    "# 56            more better bins 87k features\n",
    "# 55.5 other splits2, 41k\n",
    "# 52.6655238717 other splits, 28k features\n",
    "# 56.237862326  14k features\n",
    "# 52.1237605257  56k features\n",
    "\n",
    "import os\n",
    "os.system('say \"i have finished!\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 147.92655778]),\n",
       " 'mean_score_time': array([ 21.32155935]),\n",
       " 'mean_test_score': array([-96.58154818]),\n",
       " 'mean_train_score': array([ -1.85495607e-26]),\n",
       " 'param_BinsExtraction__bin_length': masked_array(data = [50],\n",
       "              mask = [False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'BinsExtraction__bin_length': 50}],\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_test_score': array([-90.6712437]),\n",
       " 'split0_train_score': array([ -1.73421817e-26]),\n",
       " 'split1_test_score': array([-104.79753217]),\n",
       " 'split1_train_score': array([ -1.48858479e-26]),\n",
       " 'split2_test_score': array([-94.25080693]),\n",
       " 'split2_train_score': array([ -2.34206526e-26]),\n",
       " 'std_fit_time': array([ 2.88726258]),\n",
       " 'std_score_time': array([ 0.10417926]),\n",
       " 'std_test_score': array([ 6.00543734]),\n",
       " 'std_train_score': array([  3.58738956e-27])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_\n",
    "#grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n edges: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([  316.06548606,   516.04062473,   672.03912953,   788.4017923 ,\n",
       "          912.68082233,  1063.06793716,  1221.11834124,  1370.37759073]),\n",
       " array([  308.77309614,   504.8323311 ,   661.04570736,   783.16988715,\n",
       "          921.22162422,  1080.41577968,  1247.41710179,  1393.10546197]),\n",
       " array([  259.42693236,   454.41115039,   637.10735577,   783.24697727,\n",
       "          918.05688785,  1076.03364154,  1238.7439229 ,  1383.01276065]),\n",
       " array([  284.2603614 ,   461.84852282,   647.97452156,   826.76038816,\n",
       "          980.3224361 ,  1135.55237028,  1308.37312973,  1473.53302531]),\n",
       " array([  277.75613471,   482.81300987,   649.12127803,   776.62490065,\n",
       "          913.78209436,  1076.42318956,  1235.8621241 ,  1375.33653253])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search bin subdivision so that there is highest variance\n",
    "# 1. Divide hist into bins over some rows\n",
    "# 2. Compute std -> divide bins where it's high, join/drop those where it's low\n",
    "# 3. Repeat / change rows\n",
    "kmeans = KMeans(n_clusters=8, n_jobs=-1, random_state=42)\n",
    "\n",
    "samples = X[0: 5]\n",
    "centers = []\n",
    "for i, sample in enumerate(samples):\n",
    "    # sample = sample[1672390 : -786303]\n",
    "\n",
    "    sample = sample[(sample > 0) & (sample < 1800)]\n",
    "    kmeans.fit(np.array([sample]).T)\n",
    "    centers.append(np.sort(np.array(kmeans.cluster_centers_).flatten()))\n",
    "\n",
    "    #samples[i] = sample\n",
    "    #print(str(i) + ' done')\n",
    "\n",
    "if True: # use all centers\n",
    "    values = np.array(centers).flatten()\n",
    "    values = np.sort(values)\n",
    "else: # take means of centers\n",
    "    values = np.mean(centers, axis=0)\n",
    "\n",
    "edges = [1] # leave out 0\n",
    "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
    "    edges.append(.5 * (center_1 + center_2))\n",
    "\n",
    "print('n edges: ' + str(len(edges)))\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "edges = [1,\n",
    " 175,\n",
    " 190.0,\n",
    " 200,\n",
    " 210,\n",
    " 220,\n",
    " 230.0,\n",
    " 240,\n",
    " 250.29576676838963,\n",
    " 260.59153353677925,\n",
    " 270.7998907955566,\n",
    " 281.00824805433393,\n",
    " 296.51672876785585,\n",
    " 312.4192910985714,\n",
    " 328.7096455492857,\n",
    " 345,\n",
    " 365.11915911105507,\n",
    " 385.23831822211014,\n",
    " 421.68407741265736,\n",
    " 458.1298366032046,\n",
    " 517.3519134270805,\n",
    " 576.5739902509565,\n",
    " 609.5574644576008,\n",
    " 626.0492015609229,\n",
    " 642.540938664245,\n",
    " 655.0834926946815,\n",
    " 666.5424184427528,\n",
    " 680,\n",
    " 688.7915018861073,\n",
    " 697.5830037722146,\n",
    " 705.1660075444291,\n",
    " 712.7490113166436,\n",
    " 720.3320150888583,\n",
    " 730.1660075444291,\n",
    " 740,\n",
    " 747.7243484748634,\n",
    " 755.4486969497268,\n",
    " 763.1730454245902,\n",
    " 770.8973938994536,\n",
    " 778.3608893423097,\n",
    " 785.8243847851659,\n",
    " 792.7855130483395,\n",
    " 799.7466413115131,\n",
    " 807.5810902304434,\n",
    " 816.5985296073651,\n",
    " 825.6159689842868,\n",
    " 836.8877682054391,\n",
    " 848.1595674265914,\n",
    " 863.9400863362044,\n",
    " 879.7206052458175,\n",
    " 905.8611747488183,\n",
    " 932.0017442518191,\n",
    " 950.2522817644435,\n",
    " 968.5028192770679,\n",
    " 995.0990029527295,\n",
    " 1021.6951866283912,\n",
    " 1045.622987987501,\n",
    " 1084.7830302704533,\n",
    " 1123.9430725534057,\n",
    " 1178.3353557602009,\n",
    " 1203.4127942147347,\n",
    " 1228.4902326692686,\n",
    " 1240.8414534419671,\n",
    " 1253.1926742146657,\n",
    " 1265.5438949873642,\n",
    " 1277.8951157600625,\n",
    " 1288.9475578800311,\n",
    " 1300,\n",
    " 1309.8438400564696,\n",
    " 1319.6876801129395,\n",
    " 1329.5315201694093,\n",
    " 1339.375360225879,\n",
    " 1347.7457855765365,\n",
    " 1356.1162109271943,\n",
    " 1365.7868972366239,\n",
    " 1375.4575835460535,\n",
    " 1386.4285308142548,\n",
    " 1400,\n",
    " 1408.3298109092373,\n",
    " 1416.6596218184745,\n",
    " 1424.9894327277118,\n",
    " 1433.319243636949,\n",
    " 1442,\n",
    " 1448.5,\n",
    " 1461.25,\n",
    " 1470.625,\n",
    " 1480,\n",
    " 1490.0,\n",
    " 1500,\n",
    " 1509.375,\n",
    " 1518.75,\n",
    " 1530,\n",
    " 1546.25,\n",
    " 1562.5,\n",
    " 1581.25,\n",
    " 1610,\n",
    " 1700.0]\n",
    "hists = []\n",
    "for x in X:#resample(X, n_samples=100):\n",
    "    hists.append(np.histogram(x, bins=edges)[0])\n",
    "    \n",
    "std_all = np.std(np.array(hists).flatten())\n",
    "for bin_i in range(len(hists[0])):\n",
    "    arr = [h[bin_i] for h in hists]\n",
    "    print(\"bin \"+str(bin_i)+\": \"+str(np.std(arr) / std_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "new_edges = edges # attention: 49 bins = 50 edges\n",
    "for i in range(1):\n",
    "    # compute hists for samples\n",
    "    hists = []\n",
    "    for x in X:\n",
    "        hists.append(np.histogram(x, bins=new_edges)[0])\n",
    "\n",
    "    ed = [1]\n",
    "    for bin_i in range(len(hists[0])):\n",
    "        # compute std for bin i\n",
    "        arr = [h[bin_i] for h in hists]\n",
    "        std = np.std(arr) / std_all\n",
    "        \n",
    "        if std > 0.35: # if std over threshold, split bin\n",
    "            ed.append(0.5 * (new_edges[bin_i] + new_edges[bin_i+1])) \n",
    "            \n",
    "        if std < 0.2: # if std too low, join with next bin\n",
    "            continue\n",
    "            \n",
    "        ed.append(new_edges[bin_i+1])\n",
    "        \n",
    "    new_edges = ed\n",
    "    if len(new_edges)>=120: break\n",
    "        \n",
    "print(len(new_edges))\n",
    "display(new_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

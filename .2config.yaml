# Using this to try around and get an offline score for my model
# tries to do : PolynomialFeatures and SelectKBest with linear regression

#smt run --config .config.yaml -X data/X_train.npy -y data/y_1.csv -a fit
module: ml_project.model_selection
class: GridSearchCV
params:
 est_module: ml_project.pipeline
 est_class: Pipeline
 est_params:
  # 7   mean: 0.317375552979  std: 0.0334344241206
  #  - module: ml_project.models.preprocessing
  #    class: Flatten
  #    params:
  #      dim: 2
  #  - module: sklearn.random_projection
  #    class: GaussianRandomProjection

  # full, randomSelection 1000, mean: 0.706398726566  std: 0.00709549359157
  #  - module: ml_project.models.feature_selection
  #    class: RandomSelection
  #    params:
  #      random_state: 37
  #      n_components: 1000

  # full, mean: 0.863778528597, std: 0.00486466587765
  #  - module: ml_project.models.feature_extraction
  #    class: HistExtraction
  #    params:
  #      bins: 10

  #  - module: ml_project.models.feature_selection
  #    class: RemoveEmptyValues
  #  55, mean: 0.846027470504  std: 0.016556594152
  #  60, mean: 0.845356423781  std: 0.0166878729634
  #  70, mean: 0.847869047458  std: 0.0146094943098
  # 150, mean: 0.72933134527  std: 0.0892725570146 ? probably not all data

  # kfold 3:
  #  20, delZeros: mean: 0.856737791702  std: 0.0286527402019
  #  30, delZeros: mean: 0.852930462868  std: 0.0285971083359
  #  60, delZeros: mean: 0.842957055226  std: 0.0263348549017
  #  62, delZeros: mean: 0.839027670737  std: 0.0259632079337 score 78
  #  62, mean: 0.838695320595  std: 0.0239675192377
  #  63, delZeros: mean: 0.839761129887  std: 0.0257238862591
  #  60, +feature: mean: 0.842957055226  std: 0.0263348549017
  #  30, mean only: mean: 0.859351007955  std: 0.0272902772175
  #  60, mean only: mean: 0.850134330402  std: 0.0262591635725
  #  60, normaliz: mean: 0.842965399368  std: 0.0263269986484
  # 60, norm, median: mean: 0.875549094516  std: 0.0241961177634
  #  60, mean/amax: mean: 0.871223885923  std: 0.0259421789959
  #  60, mean/std: mean: 0.866899561079  std: 0.0256796911883
  # 120, mean/std: mean: 0.851367162172  std: 0.0200311522277
  # 60 with hist bins: mean: 0.85193922784  std: 0.0261626240987
  # 62, more_features: mean: 0.849707728594  std: 0.0245772155342
  # 62: mean: 0.84319099585  std: 0.0251235415445
  # 50, 38-130: mean: 0.843574746203  std: 0.032148642953
  # 50, 50-135: mean: 0.842498541637  std: 0.0335571743274
  # 45, middle: mean: 0.843080798586  std: 0.0361421842654
  # 62, middle, more: mean: 0.843994563837  std: 0.0340749919582

  # 62, middle: mean: 0.839071004436  std: 0.0340406941865 score 74.14
  # 50, middle: mean: 0.83728244731  std: 0.0349213309324 score 73.3
  # kfold 4, mse
  # ", mse, kfold4: mean: 0.833630633033  std: 0.0292873134015
  # 40, linreg: mean: 0.83708416878  std: 0.0293496345153
  # 35, linreg: mean: 0.826372102319  std: 0.0285803311244
  # 30, linreg: mean: 0.840424834789  std: 0.0298434115336
  # 35, to 125: mean: 0.823132286683  std: 0.0302782578896
  # 35, to 120: mean: 0.821102433324  std: 0.0283531050997

  # 35, x50-130
  # ", y10-160: mean: 0.819917000775  std: 0.0317794764631
  # ", y20-160: mean: 0.81863347848  std: 0.0336067711133
  # ", y30-160: mean: 0.82138527814  std: 0.0331335295942
  # ", y20-150: mean: 0.817349782326  std: 0.0333028857287
  # ", y20-140: mean: 0.815610220185  std: 0.0361707202714
  # ", y20-130: mean: 0.800674423904  std: 0.0390894472586
  # ", y20-120: mean: 0.793969884279  std: 0.0341698808156
  # ", y20-110: mean: 0.780882079304  std: 0.0417628494162
  # ", y20-100: mean: 0.740912024865  std: 0.0360865813133 score 117.2
  # kfold6
  # ", kfold 6: mean: 0.745753485631  std: 0.0765248822274 score 117.2
  # ", y20-150: mean: 0.831773933355  std: 0.0531825669654
  # old sc, y20-160: mean: 0.831687134925  std: 0.049082418675

  # 35, to 135: mean: 0.831204378596  std: 0.0295712132338
  # 35, from 45: mean: 0.841098066165  std: 0.0299608014201
  # 35, from 55: mean: 0.831746859895  std: 0.031972550086
  # gaussian: mean: 0.835240895523  std: 0.0295815149963
  # gaussian opt10: mean: 0.842324995359  std: 0.0301100922683

  # optimiere welche bilder weggenommen werden am Anfang und Ende
  # optimiere bin_length nochmals
  #  - module: ml_project.models.feature_extraction
  #    class: BinsExtraction
  #    params:
  #      bin_length: 35
  #      images_x_from: 50
  #      images_x_to: 120
  #      images_y_from: 20
  #      images_y_to: 160
  #      del_zero_std: False
  #      more_features: False

  #  - module: ml_project.models.feature_extraction
  #    class: StatsExtraction



   - module: ml_project.models.feature_extraction
     class: GivenEdgesExtraction
    #  class: ClusteredHistExtraction
    #  params:
    #    n_clusters: 10
    #    n_samples: 3
      #  images_x_from: 50
      #  images_x_to: 130

   - module: sklearn.preprocessing
     class: StandardScaler
    #  class: Normalizer
   - module: sklearn.feature_selection
     class: VarianceThreshold
     params:
       threshold: 0.0

    # 10 3 mean: 0.881540831507  std: 0.0164859670043
    # 10 5 mean: 0.880186211752  std: 0.0187992062709
   - module: sklearn.linear_model
     class: LinearRegression
     params:
       n_jobs: -1

    # binext: mean: 0.81960217199  std: 0.0457805041688
    # clusters samples
    # adding centers
    # 3 3  mean: 0.893364733394  std: 0.0251460130129
    # 10 3 mean: 0.881540831507  std: 0.0164859670043 ??
    # 10 3 mean: 0.883364794741  std: 0.018861401846
    # 10 5 mean: 0.876532056005  std: 0.0176116013681
    # 5 20 mean: 0.885697215588  std: 0.0320907013732
    #  8 4 50 130 mean: 0.876480736452  std: 0.0197743122358
    # means of centers
    # 20 3 mean: 0.887983940253  std: 0.0205856500626
  #  - module: sklearn.kernel_ridge
  #    class: KernelRidge
  #    params:
  #      kernel: 'polynomial'

  # adding centers
  # 10 3 mean: 0.873334907888  std: 0.0230843653765
  #  - module: sklearn.ensemble
  #    class: BaggingRegressor
  #    params:
  #     #  max_samples: 0.8
  #      max_features: 0.8
  #      n_estimators: 20
  #     #  base_estimator: sklearn.svm.LinearSVR
  #      base_estimator: sklearn.linear_model.LinearRegression
  #      n_jobs: -1

    # adding centers
    # 10 3 mean: 0.089928057554  std: 0.0133574639898   ???
    #  8 3 mean: 0.0908273381295  std: 0.0217216529784
  #  - module: sklearn.svm
  #    class: LinearSVR
  #    params:
  #      C: 1.5
  #      max_iter: 1000



  # full 0.1, 100, binl 60, mean: 0.671475231808  std: 0.00069032392510
  # full 0.01, 200, binl 60, mean: 0.778631767563  std: 0.00260524577358
  # 0.01, 1000, binl 50, mean: 0.826848028644  std: 0.030524973401 score 104.47566
  #  - module: sklearn.ensemble
  #    class: GradientBoostingRegressor
  #    params:
  #     learning_rate: 0.01
  #     n_estimators: 300
  #     warm_start: True

 param_grid:
   VarianceThreshold__threshold:
     - 0.0
  #  KernelRidge__kernel:
  #    - 'polynomial'
  #  LinearSVR__dual:
  #    - True
  #    - False
  #  LinearSVR__C:
  #    - 0.5
  #    - 1.0
  #    - 1.5
  #  ClusteredHistExtraction__n_clusters:
    #  - 8
    #  - 10
  #  ClusteredHistExtraction__n_samples:
  #    - 3
    #  - 4
    #  - 4
    #  - 80
  #  HistExtraction__bins:
    #  - 10
  #  BinsExtraction__bin_length:
    #  - 25
 cv:
   module: sklearn.model_selection
   class: KFold
   params:
     n_splits: 3
     shuffle: True
     random_state: 37
